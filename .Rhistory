library(rpart)
library(mlbench)
data(Glass)
## split data into a training (2/3) and test set (1/3)
index <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
# fit the model
svm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)
# make the prediction (the dependent variable, Type, has column number 10)
svm.pred <- predict(svm.model, testset[,-10])
table(pred = svm.pred, true = testset[,10])
# The function svm() returns an object of class “svm”, which partly includes the following components:
#	SV: matrix of support vectors found;
#	labels: their labels in classification mode;
#	index: index of the support vectors in the input data (could be used e.g., for visualization)
# Other important parameters:
# 	class.weights: allows to introduce class weighing, useful for very asymmetric classes
#	cross: (default 0) for k-fold CV
# A nice tool in package e1071 is the possibility of tuning the parameters by 10-CV grid search:
mytunedsvm <- tune.svm(Type ~ ., data = trainset, gamma = 2^(-1:1), cost = 2^(2:4))
summary(mytunedsvm)
plot (mytunedsvm, transform.x=log10, xlab=expression(log[10](gamma)), ylab="C")
data = seq(1,10)
classes = c('b','b','b','b','a','a','a','a','b','b')
mysvm = svm (data, classes, type='C', kernel='linear')
pred1 = predict (mysvm, data)
table(pred1, classes)
mysvm = svm (data, classes, type='C', kernel='polynomial', degree=2)
pred2 = predict (mysvm, data)
table(pred2, classes)
mysvm = svm (data, classes, type='C', kernel='radial', gamma=0.1)
pred3 = predict (mysvm, data)
table(pred3, classes)
mysvm = svm (data, classes, type='C', kernel='radial', gamma=0.1, cost=10)
pred4 = predict (mysvm, data)
table(pred4, classes)
div.off()
dev.off()
data = seq(1,10)
classes = c('b','b','b','b','a','a','a','a','b','b')
mysvm = svm (data, classes, type='C', kernel='linear')
pred1 = predict (mysvm, data)
table(pred1, classes)
mysvm = svm (data, classes, type='C', kernel='polynomial', degree=2)
pred2 = predict (mysvm, data)
table(pred2, classes)
mysvm = svm (data, classes, type='C', kernel='radial', gamma=0.1)
pred3 = predict (mysvm, data)
table(pred3, classes)
mysvm = svm (data, classes, type='C', kernel='radial', gamma=0.1, cost=10)
pred4 = predict (mysvm, data)
table(pred4, classes)
library(rpart)
library(mlbench)
install.packages("mlbench")
library(mlbench)
data(Glass)
index <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
svm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)
svm.pred <- predict(svm.model, testset[,-10])
table(pred = svm.pred, true = testset[,10])
mytunedsvm <- tune.svm(Type ~ ., data = trainset, gamma = 2^(-1:1), cost = 2^(2:4))
summary(mytunedsvm)
plot (mytunedsvm, transform.x=log10, xlab=expression(log[10](gamma)), ylab="C")
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
Now we split the data into a training set (80%) and a test set (20%):
ntrain <- round(n*0.8) # number of training examples
ntrain <- round(n*0.8) # number of training examples
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
dev.off()
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c(’Positive Train’,’Positive Test’,’Negative Train’,’Negative Test’),
library(kernlab)
install.packages("kernlab")
library(kernlab)
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel=’vanilladot’,C=100,scaled=c())
Look and understand what svp contains
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel=’vanilladot’,C=100,scaled=c())
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel=’vanilla’,C=100,scaled=c())
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel="vanilladot",C=100,scaled=c())
svp
attributes(svp)
alpha(svp)
alphaindex(svp)
b(svp)
plot(svp,data=xtrain)
legend("topleft",c(’Positive Train’,"Positive Test","Negative Train","Negative Test"),
legend("topleft",c("Positive Train","Positive Test","Negative Train","Negative Test"),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
#First generate a set of positive and negative examples from 2 Gaussians.
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c(’Positive’,’Negative’),col=seq(2),pch=1,text.col=seq(2))
##Now we split the data into a training set (80%) and a test set (20%):
## Prepare a training and a test set ##
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
dev.off()
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c("Positive Train","Positive Test","Negative Train","Negative Test"),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
#First generate a set of positive and negative examples from 2 Gaussians.
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c(’Positive’,’Negative’),col=seq(2),pch=1,text.col=seq(2))
##Now we split the data into a training set (80%) and a test set (20%):
## Prepare a training and a test set ##
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
dev.off()
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c("Positive Train","Positive Test","Negative Train","Negative Test"),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
# load the kernlab package
library(kernlab)
# train the SVM
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel="vanilladot",C=100,scaled=c())
#Look and understand what svp contains
# General summary
svp
# Attributes that you can access
attributes(svp)
# For example, the support vectors
alpha(svp)
alphaindex(svp)
b(svp)
# Use the built-in function to pretty-plot the classifier
plot(svp,data=xtrain)
# Predict labels on test
ypred = predict(svp,xtest)
table(ytest,ypred)
# Compute accuracy
sum(ypred==ytest)/length(ytest)
# Compute at the prediction scores
ypredscore = predict(svp,xtest,type="decision")
# Check that the predicted labels are the signs of the scores
table(ypredscore > 0,ypred)
# Package to compute ROC curve, precision-recall etc...
library(ROCR)
pred <- prediction(ypredscore,ytest)
# Plot ROC curve
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
# Plot precision/recall curve
perf <- performance(pred, measure = "prec", x.measure = "rec")
plot(perf)
# Plot accuracy as function of threshold
perf <- performance(pred, measure = "acc")
plot(perf)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
library(ROCR)
install.packages("ROCR")
#First generate a set of positive and negative examples from 2 Gaussians.
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c(’Positive’,’Negative’),col=seq(2),pch=1,text.col=seq(2))
##Now we split the data into a training set (80%) and a test set (20%):
## Prepare a training and a test set ##
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
dev.off()
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c("Positive Train","Positive Test","Negative Train","Negative Test"),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
# load the kernlab package
library(kernlab)
# train the SVM
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel="vanilladot",C=100,scaled=c())
#Look and understand what svp contains
# General summary
svp
# Attributes that you can access
attributes(svp)
# For example, the support vectors
alpha(svp)
alphaindex(svp)
b(svp)
# Use the built-in function to pretty-plot the classifier
plot(svp,data=xtrain)
# Predict labels on test
ypred = predict(svp,xtest)
table(ytest,ypred)
# Compute accuracy
sum(ypred==ytest)/length(ytest)
# Compute at the prediction scores
ypredscore = predict(svp,xtest,type="decision")
# Check that the predicted labels are the signs of the scores
table(ypredscore > 0,ypred)
# Package to compute ROC curve, precision-recall etc...
library(ROCR)
pred <- prediction(ypredscore,ytest)
# Plot ROC curve
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
# Plot precision/recall curve
perf <- performance(pred, measure = "prec", x.measure = "rec")
plot(perf)
# Plot accuracy as function of threshold
perf <- performance(pred, measure = "acc")
plot(perf)
plot(perf)
library(e1071)
library(caret)
library(NLP)
library(tm)
data(iris)
iris$SpeciesClass[iris$Species=="versicolor"] <- "TRUE"
iris$SpeciesClass[iris$Species!="versicolor"] <- "FALSE"
trainPositive<-subset(iris,SpeciesClass=="TRUE")
testnegative<-subset(iris,SpeciesClass=="FALSE")
inTrain<-createDataPartition(1:nrow(trainPositive),p=0.6,list=FALSE)
trainpredictors<-trainPositive[inTrain,1:4]
trainLabels<-trainPositive[inTrain,6]
testPositive<-trainPositive[-inTrain,]
testPosNeg<-rbind(testPositive,testnegative)
testpredictors<-testPosNeg[,1:4]
testLabels<-testPosNeg[,6]
svm.model<-svm(trainpredictors,y=NULL,
type='one-classification',
nu=0.10,
scale=TRUE,
kernel="radial")
svm.predtrain<-predict(svm.model,trainpredictors)
svm.predtest<-predict(svm.model,testpredictors)
confTrain<-table(Predicted=svm.predtrain,Reference=trainLabels)
confTest<-table(Predicted=svm.predtest,Reference=testLabels)
confusionMatrix(confTest,positive='TRUE')
print(confTrain)
print(confTest)
library(e1071)
library(caret)
install.packages("caret")
library(NLP)
install.packages("NLP")
install.packages("tm")
library(e1071)
library(caret)
library(NLP)
library(tm)
data(iris)
iris$SpeciesClass[iris$Species=="versicolor"] <- "TRUE"
iris$SpeciesClass[iris$Species!="versicolor"] <- "FALSE"
trainPositive<-subset(iris,SpeciesClass=="TRUE")
testnegative<-subset(iris,SpeciesClass=="FALSE")
inTrain<-createDataPartition(1:nrow(trainPositive),p=0.6,list=FALSE)
trainpredictors<-trainPositive[inTrain,1:4]
trainLabels<-trainPositive[inTrain,6]
testPositive<-trainPositive[-inTrain,]
testPosNeg<-rbind(testPositive,testnegative)
testpredictors<-testPosNeg[,1:4]
testLabels<-testPosNeg[,6]
svm.model<-svm(trainpredictors,y=NULL,
type='one-classification',
nu=0.10,
scale=TRUE,
kernel="radial")
svm.predtrain<-predict(svm.model,trainpredictors)
svm.predtest<-predict(svm.model,testpredictors)
confTrain<-table(Predicted=svm.predtrain,Reference=trainLabels)
confTest<-table(Predicted=svm.predtest,Reference=testLabels)
confusionMatrix(confTest,positive='TRUE')
print(confTrain)
print(confTest)
library(e1071)
library(caret)
library(NLP)
library(tm)
library(e1071)
dataRaw <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data", sep=",");
summary(dataRaw)
labels <- dataRaw[,2]
data <- dataRaw[,3:32]
NObs <-  nrow(data)
NTrain <- round(NObs*0.9)
NTest <- NObs - NTrain
train <- data[1:NTrain,]
labelsTrain <- labels[1:NTrain]
test <- data[(NTrain+1):NObs,]
labelsTest <- labels[(NTrain+1):NObs]
pca <- princomp(train)
train <- predict(pca,train)[,1:2]
test <- predict(pca, test)[,1:2]
plot(train[labelsTrain=="B",1],train[labelsTrain=="B",2],col="red")
points(train[labelsTrain=="M",1],train[labelsTrain=="M",2],col="green")
points(test[labelsTest=="B",1],test[labelsTest=="B",2],col="cyan")
points(test[labelsTest=="M",1],test[labelsTest=="M",2],col="black")
trainFact=data.frame(train,y=as.factor(labelsTrain))
svmfit=svm(y~.,data=trainFact,kernel="linear",cross = 5)
print(svmfit)
out=predict(svmfit,train)
print(sum(out==labelsTrain))/NTrain
plot(svmfit,trainFact)
out=predict(svmfit,test)
print(sum(out==labelsTest))/NTest
dev.off()
demo("graphics")
update()
update.packages()
y
pandas
demo("graphics")
iris
head(iris)
demo("graphics")
x<-rnorm(50)
plot(x)
plot(x);
plot(x);
quit()
sample(1:100,5)
datos<-read.csv("2006.csv", nrows = 100000)
sample(1:100,5)
sum(sample(1:100,5))
datos<-read.csv("2006.csv", nrows = 10000)
datos<-read.csv("2006.csv", nrows = 10000)
getwd()
\q
quit()
library("xlsx")
library(rJava)
library("ggplot2")
library("lattice")
install.packages("rJava", type = "source")
library(rJava)
library("xlsx")
library("XLConnect")
install.packages("XLConnectJars")
install.packages("XLConnect")
library("XLConnect")
R.version()
R.Cersion()
R.Version()
library("XLConnect")
install.packages(c("nlme", "survival"))
dataWeek<-transform(dataWeek, week = factor(week))
d_weekday<-dataWeek[dataWeek$week=="weekday",]
d_weekend<-dataWeek[dataWeek$week=="weekend",]
steps_interval_weekday<-aggregate(steps~interval,data = d_weekday, mean)
steps_interval_weekend<-aggregate(steps~interval,data = d_weekend, mean)
par(mfrow = c(2,1))
plot(steps_interval_weekday$interval,steps_interval_weekday$steps, type = "l",
main = "Weekday", xlab = "interval", ylab = "steps")
plot(steps_interval_weekend$interval,steps_interval_weekend$steps, type = "l",
main = "Weekend",xlab = "interval", ylab = "steps")
steps_interval_weekend<-aggregate(steps~interval,data = d_weekend, mean)
d_weekday<-dataWeek[dataWeek$week=="weekday",]
d_weekend<-dataWeek[dataWeek$week=="weekend",]
dataWeek<-dataComplete
dataComplete<-data
count<-0
for (i in 1:nrow(data)){
if (is.na(data$steps[i])){
mean_value<-steps_interval[steps_interval$interval==data$interval[i],]
dataComplete[i,1]<-round(mean_value$steps)
count<-count+1
}
}
data<-read.csv("activity.csv", sep = ",")
data$date = as.Date(as.character(data$date), "%Y-%m-%d")
data<-read.csv("activity.csv", sep = ",")
data$date = as.Date(as.character(data$date), "%Y-%m-%d")
dir()
setwd("Documents/DataScience/05_reproducible_research/week2/project1/RepData_PeerAssessment1/")
data<-read.csv("activity.csv", sep = ",")
data$date = as.Date(as.character(data$date), "%Y-%m-%d")
library(ggplot2)
library(scales)
##removing missing values
x<-na.omit(data)
steps_day<-aggregate(steps~date,x,sum)
ggplot(steps_day, aes(x = date, y = steps))+
geom_bar(stat = "identity", col = "black", fill = "darkblue")+
labs(title = "Steps by date", xlab = "Date", ylab = "Steps")
round(mean(steps_day$steps))
round(median(steps_day$steps))
dataComplete<-data
count<-0
for (i in 1:nrow(data)){
if (is.na(data$steps[i])){
mean_value<-steps_interval[steps_interval$interval==data$interval[i],]
dataComplete[i,1]<-round(mean_value$steps)
count<-count+1
}
}
steps_interval<-aggregate(steps~interval,x,mean)
plot(steps_interval$interval,steps_interval$steps, type = "l", xlab = "interval", ylab = "steps")
title("Average Daily Activity Pattern")
dataComplete<-data
count<-0
for (i in 1:nrow(data)){
if (is.na(data$steps[i])){
mean_value<-steps_interval[steps_interval$interval==data$interval[i],]
dataComplete[i,1]<-round(mean_value$steps)
count<-count+1
}
}
steps_day_complete<-aggregate(steps~date,dataComplete,sum)
ggplot(steps_day_complete, aes(x = date, y = steps))+
geom_bar(stat = "identity", col = "black", fill = "darkblue")+
labs(title = "Steps by date", xlab = "Date", ylab = "Steps")
dataWeek<-dataComplete
count<-0
for (i in 1:nrow(dataWeek)){
if (weekdays(dataWeek$date[i])=="Saturday"){
dataWeek$week[i]<-"weekend"
count<-count+1
}else{
if (weekdays(dataWeek$date[i])=="Sunday"){
dataWeek$week[i]<-"weekend"
}else{
dataWeek$week[i]<-"weekday"
}
}
}
dataWeek<-transform(dataWeek, week = factor(week))
d_weekday<-dataWeek[dataWeek$week=="weekday",]
d_weekend<-dataWeek[dataWeek$week=="weekend",]
steps_interval_weekday<-aggregate(steps~interval,data = d_weekday, mean)
steps_interval_weekend<-aggregate(steps~interval,data = d_weekend, mean)
par(mfrow = c(2,1))
plot(steps_interval_weekday$interval,steps_interval_weekday$steps, type = "l",
main = "Weekday", xlab = "interval", ylab = "steps")
plot(steps_interval_weekend$interval,steps_interval_weekend$steps, type = "l",
main = "Weekend",xlab = "interval", ylab = "steps")
